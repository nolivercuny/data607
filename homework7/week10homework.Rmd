---
title: "Week 9 Homework"
author: "Nick Oliver"
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
editor_options: 
  chunk_output_type: console
---

# Week 10 Homework

## Overview

In Text Mining with R, Chapter 2 looks at Sentiment Analysis.  In this assignment, you should start by getting the primary example code from chapter 2 working in an R Markdown document.  You should provide a citation to this base code.  You’re then asked to extend the code in two ways:

Work with a different corpus of your choosing, and
Incorporate at least one additional sentiment lexicon (possibly from another R package that you’ve found through research).
As usual, please submit links to both an .Rmd file posted in your GitHub repository and to your code on rpubs.com.  You make work on a small team on this assignment

## Setup

### Load Libraries

The example code referenced below requires a number of libraries which will be premptively loaded here.

```{r results=FALSE, message=FALSE, warning=FALSE}
library(tidytext)
library(janeaustenr)
library(dplyr)
library(stringr)
library(wordcloud)
library(reshape2)
library(ggplot2)
library(tidyr)
```

## Assignment Solution

The corpus I chose to work with is the Trip Advisor Reviews Data obtained from Kaggle[^3]. 

The data consists of the text from Trip Advisor reviews and the associated numerical rating that the user gave. I am going to attempt to compare the sentiment of the words in the reviews with the ratings of the user to see if they match.


First load the data
```{r}
dataUrl <- 'https://raw.githubusercontent.com/nolivercuny/data607/master/homework7/tripadvisor_hotel_reviews.csv'
reviews <- read.csv(dataUrl)
glimpse(reviews)
```

Next tokenize the reviews
```{r}
reviewsTokenized <- reviews %>%
  mutate(row = row_number()) %>%
  unnest_tokens(word, Review)
```

```{r}
reviewsTokenized %>%
  anti_join(stop_words) %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red", "green"),
                   max.words = 100)
```

```{r}

bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

bingpositive <- get_sentiments("bing") %>%
  filter(sentiment == "positive")

negativeReviews <- reviewsTokenized %>%
  anti_join(stop_words) %>%
  semi_join(bingnegative) %>%
  group_by(Rating) %>%
  summarize(negativewords = n()) %>%
  ungroup()

positiveReviews <- reviewsTokenized %>%
  anti_join(stop_words) %>%
  semi_join(bingpositive) %>%
  group_by(Rating) %>%
  summarize(positivewords = n()) %>%
  ungroup()

reviewsJoined <- negativeReviews %>% 
  inner_join(positiveReviews)

reviewsJoined %>% 
pivot_longer(!Rating)    %>% 
  ggplot(aes(fill=name, y=value, x=Rating)) + 
    geom_bar(position="dodge", stat="identity")
```

```{r}
reviewsCounts <- reviewsTokenized %>%
    anti_join(stop_words) %>%
    group_by(row, Rating) %>% 
    tally(sort=TRUE)

reviewsCounts %>%
  ggplot(aes(y=n, x=factor(Rating), fill=Rating)) +
  geom_boxplot()
```


## Example Code

Example code was taken from chapter 2 of the book Text mining with R: A tidy approach[^1]

> I have chosen to exclude the `get_sentiments` code as it is trivial and explanatory.

This code loads the works of Jane Austen and tokenizes each word into a long dataframe.

```{r}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
```


This code loads the **nrc**  sentiment dataset from Saif Mohammad and Peter Turney[^2]. It then filters the sentiment data set to only `joy` words. Filters the Jane Austen book dataframe to only use text from the book **Emma** then joins the two data sets and counts the occurance of each word in the product of the join.
```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

Here they are loading a new sentiment dataset, chunking each book into 80 line segments, computing the sentiment of the chunk and then plotting out the sentiment. This is a graphical picture of how the sentiment changes with the narrative of the books.
```{r}
jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

This section is about comparing how the different sentiment datasets map to the text of a single book. They do the same plotting of the data as in the previous code snippet but using a single book and multiple sentiment data sets.
```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

#pride_prejudice - I have intentionally excluded this to keep the assignment cleaner

afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  pride_prejudice %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```


Here the author's compare continue their analysis by counting which positive and negative words contributed the most to the sentiment scores
```{r}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

# bing_word_counts - I have intentionally modified this from its original source

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

As a follow up to the previous section the author's demonstrate adding custom "stop words" that are specific to the given analysis. Pointing out that `miss` was the one of the most common negative sentitment words but in the context of a Jane Austen novel, `miss` is used frequently as a way of identifying a female character.
```{r}
custom_stop_words <- bind_rows(tibble(word = c("miss"),  
                                      lexicon = c("custom")), 
                               stop_words)

custom_stop_words
```

Here the author's demonstrate a word cloud as another way of visualizing the data
```{r}
tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

The final section discusses using whole sentences to perform the sentiment analysis over individual words

```{r}
#demonstrate how to tokenize by sentence
p_and_p_sentences <- tibble(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")
p_and_p_sentences$sentence[2]

# counting the chapters of each book
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())

# Find which chapters are the most negative in each book
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  slice_max(ratio, n = 1) %>% 
  ungroup()
```


## References

[^1]: Silge, J., &amp; Robinson, D. (2017). Chapter 2: Sentiment analysis with tidy data. In Text mining with R: A tidy approach. essay, O'Reilly Media.
[^2]: [NRC Word-Emotion Association Lexicon (aka EmoLex)](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)
[^3]: [Alam, M. H., Ryu, W.-J., Lee, S., 2016. Joint multi-grain topic sentiment: modeling semantic aspects for online reviews. Information Sciences 339, 206–223.](https://www.kaggle.com/andrewmvd/trip-advisor-hotel-reviews)