---
title: "Week 10 Homework"
author: "Nick Oliver"
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
editor_options: 
  chunk_output_type: console
---

# Week 10 Homework

## Overview

In Text Mining with R, Chapter 2 looks at Sentiment Analysis.  In this assignment, you should start by getting the primary example code from chapter 2 working in an R Markdown document.  You should provide a citation to this base code.  You’re then asked to extend the code in two ways:

Work with a different corpus of your choosing, and
Incorporate at least one additional sentiment lexicon (possibly from another R package that you’ve found through research).
As usual, please submit links to both an .Rmd file posted in your GitHub repository and to your code on rpubs.com.  You make work on a small team on this assignment

## Setup

### Load Libraries

The example code referenced below requires a number of libraries which will be premptively loaded here.

```{r results=FALSE, message=FALSE, warning=FALSE}
library(tidytext)
library(janeaustenr)
library(dplyr)
library(stringr)
library(wordcloud)
library(reshape2)
library(ggplot2)
library(tidyr)
library(sentimentr)
library(magrittr)
```

## Assignment Solution

The corpus I chose to work with is the Trip Advisor Reviews Data obtained from Kaggle[^3]. 

The data consists of the text from Trip Advisor reviews and the associated numerical rating that the user gave. I am going to attempt to compare the sentiment of the words in the reviews with the ratings of the user to see if they match.


First load the data
```{r}
dataUrl <- 'https://raw.githubusercontent.com/nolivercuny/data607/master/homework7/tripadvisor_hotel_reviews.csv'
reviews <- read.csv(dataUrl)
glimpse(reviews)
```

I plotted out the data and noticed that there is a large amount of extreme outliers. In order to pair down the data set I will only be analyzing reviews that contain fewer than 100 words.
```{r}
reviewsCounts <- reviews %>%
    mutate(row = row_number()) %>%
    unnest_tokens(word, Review) %>%
    anti_join(stop_words) %>%
    group_by(row, Rating) %>% 
    tally(sort=TRUE)

reviewsCounts %>%
  ggplot(aes(y=n, x=factor(Rating), fill=Rating)) +
  geom_boxplot()
```

Next tokenize the reviews
```{r}
reviewsTokenized <- reviews %>%
  filter(nchar(Review) < 100) %>%
  mutate(row = row_number()) %>%
  unnest_tokens(word, Review)
```

Categorize and display the words from the reviews in a positive/negative word cloud using the `bing` lexicon. 
```{r}
reviewsTokenized %>%
  anti_join(stop_words) %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red", "green"),
                   max.words = 100)
```

I decided to count the number of positive and negative words in each rating bracket. My assumptions was that it would should more negative than positive sentiment words in the lower rating categories and would gradually flip as the ratings went up. The bar plot validates my assumption but surprisingly it does show that positive ratings are overwhelmingly full of positive sentiment over negative sentiment where lower rating reviews are much closer. 

```{r}

bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

bingpositive <- get_sentiments("bing") %>%
  filter(sentiment == "positive")

negativeReviews <- reviewsTokenized %>%
  anti_join(stop_words) %>%
  semi_join(bingnegative) %>%
  group_by(Rating) %>%
  summarize(negativewords = n()) %>%
  ungroup()

positiveReviews <- reviewsTokenized %>%
  anti_join(stop_words) %>%
  semi_join(bingpositive) %>%
  group_by(Rating) %>%
  summarize(positivewords = n()) %>%
  ungroup()

reviewsWordCount <- reviewsTokenized %>%
  anti_join(stop_words) %>%
  group_by(Rating) %>%
  summarize(count = n()) %>%
  ungroup()

reviewsJoined <- negativeReviews %>% 
  inner_join(positiveReviews) %>%
  inner_join(reviewsWordCount) %>% 
  mutate(positivepercent = positivewords / count) %>%
  mutate(negativepercent = negativewords / count)

reviewsJoined %>% 
  select(Rating, positivepercent, negativepercent) %>%
pivot_longer(!Rating)    %>% 
  ggplot(aes(fill=name, y=value, x=Rating)) + 
    geom_bar(position="dodge", stat="identity")
```


For my secondary lexicon I decided to use the `sentimentr` library. The author of the library implemented a unique algorithm that accounts for "valence shifters" in the analysis. Valence shifters are things like negators which accounts for not falsley labeling a sentence like "I was **not** happy with the service" as positive because the word "happy" appears in it. The library also performs the analysis on sentences over words which I used in my above analysis. 

Here I use the `get_sentences()` function of the library to tokenize the reviews into sentences. I then use the `sentiment_by` function to get sentiment per rating. I then plot the sentiment per rating using the built-in `plot` function of the sentimentr library.

Overall you can see that this package produces similar results to my analysis with the `bing` lexicon, but with slightly less significant results especially when it comes to the 2-star rating. It also seems to show a more positive sentiment in 4-star reviews over 5-star reviews which my analysis with `bing` does not show.

```{r}
reviewsSentAgg <- reviews %>%
  filter(nchar(Review) < 100) %>%
    get_sentences() %$%
    sentiment_by(Review, list(Rating))
reviewsSentAgg
plot(reviewsSentAgg)
```

Finally, the `sentimentr` library offers a feature where you visually see which parts of the text were labeled as positive over negative.
```{r}
reviews %>%
    mutate(row = row_number()) %>%
    filter(row %in% sample(unique(row), 5)) %>%
    mutate(review_sentence = get_sentences(Review)) %$%
    sentiment_by(review_sentence, row) %>%
    highlight(open=FALSE, file="file.html")
```

````{=html}
```{r, echo=FALSE, results='asis'}
xfun::file_string('file.html')
```
````

## Example Code

Example code was taken from chapter 2 of the book Text mining with R: A tidy approach[^1]

> I have chosen to exclude the `get_sentiments` code as it is trivial and explanatory.

This code loads the works of Jane Austen and tokenizes each word into a long dataframe.

```{r}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)
```


This code loads the **nrc**  sentiment dataset from Saif Mohammad and Peter Turney[^2]. It then filters the sentiment data set to only `joy` words. Filters the Jane Austen book dataframe to only use text from the book **Emma** then joins the two data sets and counts the occurance of each word in the product of the join.
```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

Here they are loading a new sentiment dataset, chunking each book into 80 line segments, computing the sentiment of the chunk and then plotting out the sentiment. This is a graphical picture of how the sentiment changes with the narrative of the books.
```{r}
jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

This section is about comparing how the different sentiment datasets map to the text of a single book. They do the same plotting of the data as in the previous code snippet but using a single book and multiple sentiment data sets.
```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

#pride_prejudice - I have intentionally excluded this to keep the assignment cleaner

afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  pride_prejudice %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```


Here the author's compare continue their analysis by counting which positive and negative words contributed the most to the sentiment scores
```{r}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

# bing_word_counts - I have intentionally modified this from its original source

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

As a follow up to the previous section the author's demonstrate adding custom "stop words" that are specific to the given analysis. Pointing out that `miss` was the one of the most common negative sentitment words but in the context of a Jane Austen novel, `miss` is used frequently as a way of identifying a female character.
```{r}
custom_stop_words <- bind_rows(tibble(word = c("miss"),  
                                      lexicon = c("custom")), 
                               stop_words)

custom_stop_words
```

Here the author's demonstrate a word cloud as another way of visualizing the data
```{r}
tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

The final section discusses using whole sentences to perform the sentiment analysis over individual words

```{r}
#demonstrate how to tokenize by sentence
p_and_p_sentences <- tibble(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")
p_and_p_sentences$sentence[2]

# counting the chapters of each book
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())

# Find which chapters are the most negative in each book
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  slice_max(ratio, n = 1) %>% 
  ungroup()
```


## References

[^1]: Silge, J., &amp; Robinson, D. (2017). Chapter 2: Sentiment analysis with tidy data. In Text mining with R: A tidy approach. essay, O'Reilly Media.
[^2]: [NRC Word-Emotion Association Lexicon (aka EmoLex)](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)
[^3]: [Alam, M. H., Ryu, W.-J., Lee, S., 2016. Joint multi-grain topic sentiment: modeling semantic aspects for online reviews. Information Sciences 339, 206–223.](https://www.kaggle.com/andrewmvd/trip-advisor-hotel-reviews)