---
title: "Final Project Analysis"
author: "Nick Oliver"
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
editor_options: 
  chunk_output_type: console
---

# Exploratory Analysis

```{r}
library(DBI)
library(tidyverse)
library(tidytext)
library(cowplot)
library(stringr)
library(scales)
```

```{r}
con <- dbConnect(RSQLite::SQLite(), dbname = "final_project.db")
wiredArticlesRawDf <- dbGetQuery(con, "SELECT * FROM Articles;")
glimpse(wiredArticlesRawDf)
dbDisconnect(con)
```


- cleanup data

```{r}
wiredArticlesDf <- wiredArticlesRawDf %>% 
  filter(!is.na(body)) %>%
  mutate(publishDate=as.Date(publishDate, format = "%m.%d.%Y")) %>%
  mutate(publishYear=as.numeric(format(publishDate, "%Y")))
```

Due to the random sampling of articles I scraped, I may have a varying amounts of articles for each year. To account for that I will sample a number of articles from each year where the sample size is the number of articles in the year with the least amount of articles
```{r}
minSampleSize <- wiredArticlesDf %>% 
  group_by(publishYear) %>%
  count() %>%
  min()
articlesGrouped <- wiredArticlesDf %>% 
  group_by(publishYear) %>% 
  filter(n() > 20) %>%
  slice_sample(n = 20) #temp

articlesGrouped %>% count()
```

Number of unique words in the entire year

```{r}
totalWords <- articlesGrouped %>% 
  unnest_tokens(word,body,token="words", strip_punct=TRUE) %>%
  count(publishYear) %>%
  rename(totalWords = n)

totalPlot <- totalWords %>%
  ggplot(aes(x=publishYear, y=totalWords)) +
  geom_line()
```


number of unique words in the entire year
```{r}
totalDistinctWords <- articlesGrouped %>% 
    unnest_tokens(word,body,token="words", strip_punct=TRUE) %>%
    group_by(publishYear) %>%
    mutate(uniqueWordsCount = n_distinct(word)) %>%
    select(publishYear, uniqueWordsCount) %>%
    distinct()

distinctPlot <- totalDistinctWords %>%
  ggplot(aes(x=publishYear, y=uniqueWordsCount)) +
  geom_line()

```

```{r}
meanCountByArticleByYear <- wiredArticlesDf %>% 
  group_by(publishYear, id) %>% 
  unnest_tokens(word,body,token="words", strip_punct=TRUE) %>%
  count() %>%
  ungroup() %>%
  group_by(publishYear) %>%
  summarise(avg = mean(n))
meanCountPlot <- meanCountByArticleByYear %>%
  ggplot(aes(x=publishYear, y=avg)) +
  geom_line()
```

Here we can see that there is a distinct rise in article length on average, in total word count and distinct word count after the year 2004. 
```{r}
plot_grid(totalPlot, distinctPlot,meanCountPlot, labels = "AUTO")
```

Finally to attempt approximate the 'on subject-ness' of articles of time I will attempt to use Latent Dirichlet allocation. According to https://www.tidytextmining.com/topicmodeling.html Tidy Text Mining LDA is one of the most common algorithms for associating words with topics. My intuition is that if a subset of articles are more distinctly bucketed into a topic for a given category and year, the article would be considered less "rambling" and more concise, conveying more relevant information information and not padded with irrelevant information for ad views.


```{r}
runLda <- function(year){
  by_article <- wiredArticlesDf %>% 
  filter(publishYear == year & category == 'Science') %>%
  group_by(subject)
  by_article_word <- by_article %>% 
  unnest_tokens(word, body)


  # find document-word counts
  word_counts <- by_article_word %>%
    anti_join(stop_words) %>%
    count(subject, word, sort = TRUE) %>%
    ungroup()
  
  
  articles_dtm <- word_counts %>%
    cast_dtm(subject, word, n)
  articles_dtm

articles_lda <- LDA(articles_dtm, k = 10, control = list(seed = 1234))


articles_topics <- tidy(articles_lda, matrix = "beta")


top_terms <- articles_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)


# print(top_terms %>%
#   mutate(term = reorder_within(term, beta, topic)) %>%
#   ggplot(aes(beta, term, fill = factor(topic))) +
#   geom_col(show.legend = FALSE) +
#   facet_wrap(~ topic, scales = "free") +
#   scale_y_reordered())

chapters_gamma <- tidy(articles_lda, matrix = "gamma")



# print(chapters_gamma %>%
#   mutate(subject = reorder(document, gamma * topic)) %>%
#   ggplot(aes(factor(topic), gamma)) +
#   geom_boxplot() +
#   facet_wrap(~ subject) +
#   labs(x = "topic", y = expression(gamma)))

chapter_classifications <- chapters_gamma %>%
  group_by(document) %>%
  slice_max(gamma) %>%
  ungroup()


book_topics <- chapter_classifications %>%
  count(document, topic) %>%
  group_by(document) %>%
  slice_max(n, n = 1) %>% 
  ungroup() %>%
  transmute(consensus = document, topic)

chapter_classifications %>%
  inner_join(book_topics, by = "topic") %>%
  filter(document != consensus)

assignments <- augment(articles_lda, data = articles_dtm)

assignments <- assignments %>%
  rename(subject = document) %>%
  inner_join(book_topics, by = c(".topic" = "topic"))




# print(assignments %>%
#   count(subject, consensus, wt = count) %>%
#   mutate(across(c(subject, consensus), ~str_wrap(., 20))) %>%
#   group_by(subject) %>%
#   mutate(percent = n / sum(n)) %>%
#   ggplot(aes(consensus, subject, fill = percent)) +
#   geom_tile() +
#   scale_fill_gradient2(high = "darkred", label = percent_format()) +
#   theme_minimal() +
#   theme(axis.text.x = element_text(angle = 90, hjust = 1),
#         panel.grid = element_blank()) +
#   labs(x = "Articles words were assigned to",
#        y = "Articles words came from",
#        fill = "% of assignments"))
# 
# print(assignments)

wrong_words <- assignments %>%
  filter(subject != consensus) 

nrow(wrong_words)
}
```

```{r}
wrongWords <- data.frame(year=integer(), wrongWords=integer())
for(var in 1998:2021){
  r <- runLda(var)
  wrongWords <- wrongWords %>% add_row(year = var, wrongWords = r)
}

```
Interestingly plotting the wrong words does not display an obvious trend. 
```{r}
wrongWords %>% ggplot(aes(x=year, y=wrongWords)) + geom_line()
```

