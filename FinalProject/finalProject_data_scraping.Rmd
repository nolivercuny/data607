---
title: "Final Project Data Scraping"
author: "Nick Oliver"
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
editor_options: 
  chunk_output_type: console
---

# Scraping Articles

## Loading Libraries

I am loading the following libraries

  - tidyverse
    - contains the `rvest` library used for web scraping the Wired.com source data
  - odbc
    - used to connect to Azure SQL server for long-term storage of the data

```{r}
library(tidyverse)
library(rvest)
library(odbc)
```

## Web Scraping
We will start with the sitemap. Luckily it is a very simple page which just contains a very long list of links to article archives.
```{r}
siteMapUrl <- "https://www.wired.com/sitemap/"
siteMapHtml <- read_html(siteMapUrl)
```

To select the links to the archives I simply need to select the parent div which wraps the archive list then grab the `a` elements
```{r}
archiveLinkNodes <- siteMapHtml %>%
  html_nodes(".sitemap__section-archive > ul > li > a")
```

I end up with `r length(archiveLinkNodes)` archive links. Each archive contains multiple articles so to simplify the process and decrease the risk of being blocked I will randomly sample the archive list. 

I do want to make that I sample enough data because I want the articles to span a large  time span
```{r}
set.seed(1972)

sampledArchiveLinks <- sample(archiveLinkNodes, 150) %>%
  html_text()
```

Check the years to make sure we have a good sample. It appears the sample includes a link from every year that the Wired.com has published articles online which is an ideal result.
```{r}
sampledArchiveLinks %>%
  stringi::stri_extract_all_regex("(?<=year=)[0-9]+") %>%
  unlist() %>%
  unique() %>%
  parse_number() %>%
  sort()
```

Next I need to use the links to randomly select a subset of articles from the archive. Each link takes the user to another page with a list of articles that were published on the year, month and week listed in the link.

```{r}
links <- c()
for (archiveLink in sampledArchiveLinks[1:2]) {
  siteMapHtml <- read_html(archiveLink)
  archiveLinkNodes <- siteMapHtml %>% 
    html_nodes(".sitemap__section-archive > ul > li > a") %>%
    html_text()
  links <- c(links, archiveLinkNodes)
}
```

Now we have the links it's time to extract the data

```{r}
extractArticleContent <- function(html) {
  publishDate <- html %>%
  html_node(xpath = '//*[@data-testid="ContentHeaderPublishDate"]') %>% 
  html_text()


author <- html %>%
  html_node(xpath = '//*[@data-testid="BylineName"]') %>% 
  html_text()


subject <- html %>%
  html_node(xpath = '//*[@data-testid="ContentHeaderHed"]') %>% 
  html_text()


byLine <- html %>%
  html_node(xpath = '//*[@data-testid="ContentHeaderAccreditation"]/div') %>% 
  html_text()

category <- html %>% 
  html_nodes(".rubric__link") %>%
  html_text()

articleBody <- html %>% 
  html_node(".body__inner-container") %>% 
  html_text()

c(publishDate, author, subject, byLine, category, articleBody)
}
```

```{r}
articlesDf <- data.frame(publishDate = character(), author = character(), subject = character(), byLine = character(), category = character(), body = character())
colnames(articlesDf) <- c('publishDate', 'author', 'subject', 'byLine', 'category', 'body')
```

```{r}


for (link in sample(links,3)) {
  tryCatch(expr={
      html <- read_html(link)
  data <- extractArticleContent(html)
  articlesDf <- articlesDf %>% 
    add_row(publishDate = data[1], author = data[2], subject = data[3], byLine = data[4], category = data[5], body = data[6])
  },error=function(e){
                message(paste("URL does not seem to exist:", link))
            message("Here's the original error message:")
            message(e)
  },finally = {
                message(paste("Processed URL:", link))
            
  })
Sys.sleep(sample(1:5, 1))
}



```

Now that I have the data from a large amount of articles spanning a large time range, I will store that data in a database so it can be cleaned and analyzed


## Database

```{r}
sqlServer <- Sys.getenv("CUNY_DB_CONN")  #Enter Azure SQL Server
sqlDatabase <- "cuny"                #Enter Database Name
sqlUser <- "data607_final_project_writer"             #Enter the SQL User ID
sqlPassword <- Sys.getenv("CUNY_DB_WRITER_USER_PWD")      #Enter the User Password
sqlDriver <- "ODBC Driver 17 for SQL Server"        #Leave this Drive Entry

connectionStringSQL <- paste0(
  "Driver=", sqlDriver, 
  ";Server=tcp:", sqlServer, 
  ";Database=", sqlDatabase, 
  ";Uid=", sqlUser, 
  ";Pwd=", sqlPassword,
  ";Encrypt=yes",
  ";Port=1433",
  ";TrustServerCertificate=no",
  ";Connection Timeout=30")
```

```{r}
sqlQuery <- "SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_TYPE='BASE TABLE'"
my_connection <- dbConnect(odbc(),
                           Driver = "ODBC Driver 17 for SQL Server",
                           Server="tcp:cunynoliver.database.windows.net,14323",
                           database = "cuny",
                           uid = sqlUser,
                           pwd = sqlPassword)

close(my_connection)
# conn <- RODBC::odbcDriverConnect(connectionStringSQL)
# sqlDataFrame <- RODBC::sqlQuery(conn, sqlQuery)
# RODBC::close(conn)

```

