---
title: "Final Project Data Scraping"
author: "Nick Oliver"
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
editor_options: 
  chunk_output_type: console
---

# Scraping Articles

## Loading Libraries

I am loading the following libraries

  - tidyverse
    - contains the `rvest` library used for web scraping the Wired.com source data
  - DBI
    - used to connect to Azure SQL server for long-term storage of the data

```{r}
library(tidyverse)
library(rvest)
library(DBI)
library(RCurl)
```

## Web Scraping

We will start with the sitemap. Luckily it is a very simple page which just contains a very long list of links to article archives.
```{r}
siteMapUrl <- "https://www.wired.com/sitemap/"
siteMapHtml <- read_html(siteMapUrl)
```

To select the links to the archives I simply need to select the parent div which wraps the archive list then grab the `a` elements
```{r}
archiveLinkNodes <- siteMapHtml %>%
  html_nodes(".sitemap__section-archive > ul > li > a")
```

I end up with `r length(archiveLinkNodes)` archive links. Each archive contains multiple articles so to simplify the process and decrease the risk of being blocked I will randomly sample the archive list. 

I do want to make that I sample enough data because I want the articles to span a large  time span
```{r}
set.seed(1972)

sampledArchiveLinks <- sample(archiveLinkNodes, 150) %>%
  html_text()
```

Check the years to make sure we have a good sample. It appears the sample includes a link from every year that the Wired.com has published articles online which is an ideal result.
```{r}
sampledArchiveLinks %>%
  stringi::stri_extract_all_regex("(?<=year=)[0-9]+") %>%
  unlist() %>%
  unique() %>%
  parse_number() %>%
  sort()
```

Next I need to use the links to randomly select a subset of articles from the archive. Each link takes the user to another page with a list of articles that were published on the year, month and week listed in the link.

```{r}
links <- list()
for (archiveLink in sampledArchiveLinks) {
  tryCatch(expr={
    siteMapHtml <- read_html(archiveLink)
    archiveLinkNodes <- siteMapHtml %>% 
      html_nodes(".sitemap__section-archive > ul > li > a") %>%
      html_text()
    links[[archiveLink]] <- archiveLinkNodes
  },error=function(e){
    message(paste("Failed to retrieve data for link ", archiveLink))
    message(e)
  },finally = {
    message(paste("Processed URL ", archiveLink))
    Sys.sleep(sample(1:5, 1))
  })
}
write.csv(links, file="final_project_links.csv")
```

```{r}
library(jsonlite)
library(purrr)
library(data.table)

dt_list <- map(links, as.data.table)
dt <- rbindlist(dt_list, fill = TRUE, idcol = T)

dt <- dt %>% rename(source = .id, link = V1)
```


Now we have the links it's time to extract the data

```{r}
missingConvert <- function(variable){
  if(length(variable) == 0) NA else variable
}

extractArticleContent <- function(html) {
  publishDate <- html %>%
  html_node(xpath = '//*[@data-testid="ContentHeaderPublishDate"]') %>% 
  html_text() %>%
  convertToEmptyString()


author <- html %>%
  html_node(xpath = '//*[@data-testid="BylineName"]') %>% 
  html_text() %>%
  convertToEmptyString()


subject <- html %>%
  html_node(xpath = '//*[@data-testid="ContentHeaderHed"]') %>% 
  html_text() %>%
  convertToEmptyString()


byLine <- html %>%
  html_node(xpath = '//*[@data-testid="ContentHeaderAccreditation"]/div') %>% 
  html_text() %>%
  convertToEmptyString()

category <- html %>% 
  html_nodes(".rubric__link") %>%
  html_text() %>%
  convertToEmptyString()

articleBody <- html %>% 
  html_node(".body__inner-container") %>% 
  html_text() %>%
  convertToEmptyString()



c(publishDate, author, subject, byLine, category, articleBody)
}
```

```{r}
articlesDf <- data.frame(publishDate = character(), author = character(), subject = character(), byLine = character(), category = character(), body = character(), sourceUrl = character(), retrievedAt = character())
```

```{r}
links <- dbGetQuery(con, "select lx.link
from LinksXref lx
left join Articles a on a.sourceUrl = lx.link
where a.sourceUrl is null;") 
```

```{r}
#Open DB connection
con <- dbConnect(RSQLite::SQLite(), dbname = "final_project.db")

#Get list of articles we haven't retrieved yet
links <- dbGetQuery(con, "select lx.link
from LinksXref lx
left join Articles a on a.sourceUrl = lx.link
where a.sourceUrl is null;") 

#Create empty DataFrame to store articles with columns that match our DB table
articlesDf <- data.frame(publishDate = character(), author = character(), subject = character(), byLine = character(), category = character(), body = character(), sourceUrl = character(), retrievedAt = character())

#Grab random sample of articles
for(article in sample(links$link,100)){
    tryCatch(expr={
      html <- read_html(article)
      data <- extractArticleContent(html)
      articlesDf <- articlesDf %>% 
      add_row(publishDate = data[1], author = data[2], subject = data[3], byLine = data[4], category = data[5], body = data[6], sourceUrl = article, retrievedAt =format(Sys.time(), "%Y-%m-%d %H:%M:%S %Z"))
    },error=function(e){
      message(paste("Failed on article ", article))
      message(e)
    },finally = {
      message(paste("Processed URL:", article))
      Sys.sleep(sample(1:5, 1))
    })
}
#Store data in a csv as a backup
write.csv(articlesDf, file = paste0("articles-",as.numeric(as.POSIXct(Sys.time())),".csv"))
#Write articles out to table
dbWriteTable(con,"Articles",articlesDf, append=TRUE)
#Disconnect from DB
dbDisconnect(con)
```

This was from when I originally pulled all of the links from the sitemap. Now that I've inserted everything into a table I can use SQL to filter articles I've already retrieved.
```{r}
# for (link in links[1:25]) {
#     for(article in sample(link,10)){
#       tryCatch(expr={
#         html <- read_html(article)
#         data <- extractArticleContent(html)
#         articlesDf <- articlesDf %>% 
#         add_row(publishDate = data[1], author = data[2], subject = data[3], byLine = data[4], category = data[5], body = data[6], sourceUrl = article, retrievedAt =format(Sys.time(), "%Y-%m-%d %H:%M:%S %Z"))
#       },error=function(e){
#         message(paste("Failed on article ", article))
#         message(e)
#       },finally = {
#         message(paste("Processed URL:", article))
#         Sys.sleep(sample(1:5, 1))
#       })
#     }
# }
```

Now that I have the data from a large amount of articles spanning a large time range, I will store that data in a database so it can be cleaned and analyzed

## Database

Establish connection
```{r}

#For testing use in memory DB
# con <- dbConnect(RSQLite::SQLite(), dbname = ":memory:")

#Uncomment to manipulate real DB
con <- dbConnect(RSQLite::SQLite(), dbname = "final_project.db")
```

```{r}
dbWriteTable(con,"LinksXref",dt, append=TRUE)
```

Insert records
```{r}
dbWriteTable(con,"Articles",articlesDf, append=TRUE)
```

Make sure our data was inserted correctly
```{r}
dbGetQuery(con, "SELECT * FROM Articles")
```

```{r}
dbDisconnect(con)
```

```{r}
html <- read_html("https://www.wired.com/2004/01/rants-raves-19-218/")
extractArticleContent(html)
```



